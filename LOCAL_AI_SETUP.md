# ü§ñ Configura√ß√£o de IA Local - Sentinel-Pelanda

## üìã Vis√£o Geral

O Sentinel-Pelanda utiliza **exclusivamente IA Local** usando [Ollama](https://ollama.ai), garantindo processamento offline e m√°xima privacidade dos dados.

## üéØ Benef√≠cios da IA Local

- ‚úÖ **Processamento Offline** - Sem depend√™ncia de APIs externas
- ‚úÖ **Privacidade Total** - Dados nunca saem do seu ambiente
- ‚úÖ **Custo Zero** - Sem taxas de API
- ‚úÖ **Lat√™ncia Baixa** - Processamento local mais r√°pido
- ‚úÖ **Controle Total** - Modelos e configura√ß√µes personaliz√°veis
- ‚úÖ **Seguran√ßa M√°xima** - Sem transmiss√£o de dados sens√≠veis

## üöÄ Instala√ß√£o R√°pida

### 1. Pr√©-requisitos

```bash
# Docker e Docker Compose
docker --version
docker-compose --version

# GPU (opcional, mas recomendado)
nvidia-smi
```

### 2. Configura√ß√£o Autom√°tica

```bash
# Dar permiss√£o ao script
chmod +x setup-local-ai.sh

# Executar setup
./setup-local-ai.sh
```

### 3. Iniciar Servi√ßos

```bash
# Iniciar todos os servi√ßos
docker-compose up -d

# Verificar status
docker-compose ps
```

## üîß Configura√ß√£o Manual

### 1. Instalar Ollama

#### Linux/macOS
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Windows
```bash
# Baixar de https://ollama.ai/download
```

### 2. Baixar Modelos

```bash
# Modelo padr√£o (recomendado)
ollama pull llama2

# Modelos alternativos
ollama pull codellama:7b    # Para an√°lise de c√≥digo
ollama pull mistral:7b      # Equilibrado
ollama pull neural-chat:7b  # Conversacional
```

### 3. Configurar Vari√°veis de Ambiente

```bash
# Backend/.env
LOCAL_AI_URL=http://localhost:11434
LOCAL_AI_MODEL=llama2
```

## üìä Modelos Dispon√≠veis

### Modelos Recomendados

| Modelo | Tamanho | RAM | Uso Recomendado |
|--------|---------|-----|-----------------|
| `llama2` | 7B | 8GB | An√°lise geral |
| `codellama:7b` | 7B | 8GB | An√°lise de c√≥digo |
| `mistral:7b` | 7B | 8GB | Equilibrado |
| `neural-chat:7b` | 7B | 8GB | Conversacional |

### Modelos Leves (RAM limitada)

| Modelo | Tamanho | RAM | Uso |
|--------|---------|-----|-----|
| `llama2:3b` | 3B | 4GB | B√°sico |
| `mistral:3b` | 3B | 4GB | R√°pido |
| `phi:2.7b` | 2.7B | 3GB | Muito leve |

## üéõÔ∏è Configura√ß√£o Avan√ßada

### 1. Otimiza√ß√£o de Performance

```bash
# Configurar GPU (se dispon√≠vel)
export CUDA_VISIBLE_DEVICES=0

# Ajustar mem√≥ria
export OLLAMA_HOST=0.0.0.0:11434
```

### 2. Modelos Customizados

```bash
# Criar modelo customizado
ollama create security-analyst -f Modelfile

# Modelfile exemplo:
FROM llama2
SYSTEM Voc√™ √© um analista de seguran√ßa cibern√©tica especializado em an√°lise de logs.
```

### 3. Configura√ß√£o de Rede

```dockerfile
# docker-compose.yml
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

## üîç Verifica√ß√£o e Testes

### 1. Verificar Status

```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags

# Verificar modelos dispon√≠veis
ollama list
```

### 2. Teste de Performance

```bash
# Teste b√°sico
ollama run llama2 "Analise este log de seguran√ßa: [seu log aqui]"

# Teste de velocidade
time ollama run llama2 "Hello, world!"
```

### 3. Teste na Interface Web

1. Acesse http://localhost:3000
2. V√° para "An√°lise de Logs"
3. Selecione "IA Local (Ollama)" como provedor
4. Cole um log de teste
5. Clique em "Analisar Log"

## üõ†Ô∏è Troubleshooting

### Problemas Comuns

#### 1. Ollama n√£o inicia
```bash
# Verificar logs
docker-compose logs ollama

# Reiniciar servi√ßo
docker-compose restart ollama
```

#### 2. Modelo n√£o encontrado
```bash
# Baixar modelo novamente
docker-compose exec ollama ollama pull llama2

# Verificar modelos dispon√≠veis
docker-compose exec ollama ollama list
```

#### 3. Erro de mem√≥ria
```bash
# Usar modelo menor
export LOCAL_AI_MODEL=llama2:3b

# Aumentar swap (Linux)
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

#### 4. GPU n√£o detectada
```bash
# Verificar drivers NVIDIA
nvidia-smi

# Instalar drivers se necess√°rio
# Ubuntu: sudo apt install nvidia-driver-xxx
# CentOS: sudo yum install nvidia-driver
```

### Logs e Debug

```bash
# Ver logs do Ollama
docker-compose logs -f ollama

# Ver logs do backend
docker-compose logs -f backend

# Testar conectividade
curl http://localhost:11434/api/generate -d '{"model":"llama2","prompt":"test"}'
```

## üìà Monitoramento

### 1. M√©tricas de Performance

```bash
# Uso de mem√≥ria
docker stats ollama

# Logs de performance
docker-compose logs ollama | grep "duration"
```

### 2. Health Check

```bash
# Verificar sa√∫de da aplica√ß√£o
curl http://localhost:3001/api/health

# Verificar provedores dispon√≠veis
curl http://localhost:3001/api/health | jq '.ai.providers'
```

## üîí Seguran√ßa

### 1. Configura√ß√µes de Seguran√ßa

```bash
# Restringir acesso √† API
export OLLAMA_HOST=127.0.0.1:11434

# Usar HTTPS (se necess√°rio)
# Configurar proxy reverso com SSL
```

### 2. Isolamento de Rede

```dockerfile
# docker-compose.yml
services:
  ollama:
    networks:
      - internal
    expose:
      - "11434"

networks:
  internal:
    internal: true
```

## üöÄ Deploy em Produ√ß√£o

### 1. Configura√ß√£o de Produ√ß√£o

```bash
# Usar modelo otimizado
export LOCAL_AI_MODEL=llama2:7b

# Configurar recursos
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d
```

### 2. Backup de Modelos

```bash
# Backup dos modelos
docker run --rm -v ollama_data:/root/.ollama -v $(pwd):/backup alpine tar czf /backup/ollama-models.tar.gz /root/.ollama

# Restore dos modelos
docker run --rm -v ollama_data:/root/.ollama -v $(pwd):/backup alpine tar xzf /backup/ollama-models.tar.gz -C /
```

## üìö Recursos Adicionais

- [Documenta√ß√£o Ollama](https://ollama.ai/docs)
- [Modelos Dispon√≠veis](https://ollama.ai/library)
- [Performance Tips](https://github.com/ollama/ollama/blob/main/docs/troubleshooting.md)
- [GPU Setup](https://ollama.ai/docs/gpu)

## ü§ù Suporte

- **Issues**: Abra uma issue no reposit√≥rio
- **Discord**: [Ollama Community](https://discord.gg/ollama)
- **Documenta√ß√£o**: [Sentinel-Pelanda Docs](https://github.com/seu-usuario/sentinel-pelanda)

---

**Sentinel-Pelanda** - IA Local Exclusiva para An√°lise de Seguran√ßa Cibern√©tica 